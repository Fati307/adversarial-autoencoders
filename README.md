# adversarial-autoencoders

Ce dÃ©pÃ´t contient le code source de trois implÃ©mentations d'Adversarial Autoencoders pour l'apprentissage de reprÃ©sentations latentes sur MNIST, dÃ©veloppÃ©es avec PyTorch.

## Table des matiÃ¨res

- [PrÃ©requis](#prÃ©requis)
- [DÃ©marrage rapide](#dÃ©marrage-rapide)
  - [Cloner le dÃ©pÃ´t](#cloner-le-dÃ©pÃ´t)
  - [Configuration de l'environnement](#configuration-de-lenvironnement)
  - [PrÃ©paration des donnÃ©es](#prÃ©paration-des-donnÃ©es)
  - [ExÃ©cution de l'entraÃ®nement](#exÃ©cution-de-lentraÃ®nement)
- [Architecture des ModÃ¨les](#architecture-des-modÃ¨les)
- [Visualisation et RÃ©sultats](#visualisation-et-rÃ©sultats)
- [ğŸ“„ Document de RÃ©fÃ©rence Technique](#-document-de-rÃ©fÃ©rence-technique)

---

## 1. PrÃ©requis

Avant de commencer, assurez-vous d'avoir les Ã©lÃ©ments suivants installÃ©s et configurÃ©s :

- **Python 3.8** ou supÃ©rieur
- **PyTorch 2.0+** avec support CUDA (recommandÃ©)
- **pip** ou **conda** pour la gestion des packages
- **Git** pour cloner le dÃ©pÃ´t
- **GPU NVIDIA** (optionnel mais fortement recommandÃ© pour l'entraÃ®nement)

**Packages Python requis :**
- `torch>=2.0.0`
- `torchvision>=0.15.0`
- `numpy>=1.24.0`
- `matplotlib>=3.7.0`
- `scikit-learn>=1.3.0`
- `tqdm>=4.65.0`

---

## 2. DÃ©marrage rapide

### Cloner le dÃ©pÃ´t

**Avec SSH :**
```bash
git clone git@github.com:Fati307/adversarial-autoencoders.git
cd adversarial-autoencoders
```

**Avec HTTPS :**
```bash
git clone https://github.com/Fati307/adversarial-autoencoders.git
cd adversarial-autoencoders
```

### Configuration de l'environnement

Installez les dÃ©pendances depuis la racine du projet :

```bash
pip install -r requirements.txt
```

**Contenu du fichier `requirements.txt` :**
```
torch>=2.0.0
torchvision>=0.15.0
numpy>=1.24.0
matplotlib>=3.7.0
scikit-learn>=1.3.0
tqdm>=4.65.0
```

### PrÃ©paration des donnÃ©es

Le projet utilise le dataset MNIST organisÃ© en structure `ImageFolder`. Placez vos donnÃ©es dans le dossier suivant :

```
data/
â””â”€â”€ mnist/
    â”œâ”€â”€ 0/
    â”‚   â”œâ”€â”€ img1.png
    â”‚   â”œâ”€â”€ img2.png
    â”‚   â””â”€â”€ ...
    â”œâ”€â”€ 1/
    â”‚   â”œâ”€â”€ img1.png
    â”‚   â””â”€â”€ ...
    â””â”€â”€ ...
```

Le fichier `dataset.py` Ã  la racine du projet gÃ¨re automatiquement le chargement des donnÃ©es pour tous les modÃ¨les.

### ExÃ©cution de l'entraÃ®nement

**Pour AAE (Adversarial Autoencoder) :**
```bash
cd AAE
python train_aae.py
```

**Pour CAAE (Conditional Adversarial Autoencoder) :**
```bash
cd CAAE
python train_caae.py
```

**Pour Hybrid CAAE :**
```bash
cd CAAEHybrid
python train_h_caae.py
```

Les rÃ©sultats (images de reconstruction, visualisations du latent space) seront sauvegardÃ©s dans les dossiers `result_aae/`, `result_caae/`, et `result_h_caae/`.

---

## 3. Architecture des ModÃ¨les

### Structure du projet

```
adversarial-autoencoders/
â”‚
â”œâ”€â”€ dataset.py                     # Dataset commun pour les 3 modÃ¨les
â”œâ”€â”€ requirements.txt               # DÃ©pendances Python
â”œâ”€â”€ README.md                      # Documentation du projet
â”‚
â”œâ”€â”€ AAE/                           # Adversarial Autoencoder (Non-conditionnel)
â”‚   â”œâ”€â”€ config_aae.py              # HyperparamÃ¨tres AAE
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ encoder.py             # Encodeur CNN
â”‚   â”‚   â”œâ”€â”€ decoder.py             # DÃ©codeur CNN transposÃ©
â”‚   â”‚   â”œâ”€â”€ discriminator_img.py  # Discriminateur d'images
â”‚   â”‚   â””â”€â”€ discriminator_z.py    # Discriminateur latent
â”‚   â”œâ”€â”€ losses_aae.py              # Fonctions de perte
â”‚   â”œâ”€â”€ train_aae.py               # Script d'entraÃ®nement
â”‚   â””â”€â”€ utils_aae.py               # Fonctions utilitaires (affichage, init)
â”‚
â”œâ”€â”€ CAAE/                          # Conditional Adversarial Autoencoder
â”‚   â”œâ”€â”€ config_caae.py             # HyperparamÃ¨tres CAAE
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ encoder.py             # Encodeur VAE-style avec reparamÃ©trisation
â”‚   â”‚   â”œâ”€â”€ decoder.py             # DÃ©codeur conditionnel (z + classe)
â”‚   â”‚   â”œâ”€â”€ discriminator_img.py  # Discriminateur d'images
â”‚   â”‚   â””â”€â”€ discriminator_z.py    # Discriminateur latent
â”‚   â”œâ”€â”€ losses_caae.py             # Perte reconstruction + KL + adversarial + classification
â”‚   â”œâ”€â”€ train_caae.py              # Script d'entraÃ®nement avec conditionnement
â”‚   â””â”€â”€ utils_caae.py              # One-hot encoding & visualisation
â”‚
â””â”€â”€ CAAEHybrid/                    # Hybrid Conditional AAE
    â”œâ”€â”€ config_h_caae.py           # HyperparamÃ¨tres Hybrid
    â”œâ”€â”€ models/
    â”‚   â”œâ”€â”€ encoder.py             # Encodeur conditionnel (image + classe â†’ z)
    â”‚   â”œâ”€â”€ decoder.py             # DÃ©codeur conditionnel (z + classe â†’ image)
    â”‚   â”œâ”€â”€ discriminator_img.py  # Double tÃªte: real/fake + classification
    â”‚   â””â”€â”€ discriminator_z.py    # Discriminateur latent conditionnel
    â”œâ”€â”€ losses_h_caae.py           # Perte hybride: L1 + adversarial + classification
    â”œâ”€â”€ train_h_caae.py            # EntraÃ®nement multi-objectif
    â””â”€â”€ utils_h_caae.py            # Class maps & utilitaires avancÃ©s
```

### Comparaison des modÃ¨les

| CaractÃ©ristique | AAE | CAAE | Hybrid CAAE |
|-----------------|-----|------|-------------|
| **Type** | Non-conditionnel | Conditionnel VAE | Conditionnel Hybride |
| **Conditionnement** | âŒ | âœ… Labels de classe | âœ… Labels de classe |
| **RÃ©gularisation latente** | Adversarial | KL + Adversarial | Adversarial |
| **Discriminateur d'images** | âœ… | âœ… | âœ… + Classification auxiliaire |
| **Discriminateur latent** | âœ… | âœ… | âœ… Conditionnel |
| **GÃ©nÃ©ration contrÃ´lÃ©e** | AlÃ©atoire | Par classe | Par classe |
| **ComplexitÃ©** | Faible | Moyenne | Ã‰levÃ©e |
| **Cas d'usage** | Exploration non-supervisÃ©e | GÃ©nÃ©ration contrÃ´lÃ©e | Applications avancÃ©es |

---

## 4. Visualisation et RÃ©sultats

Chaque script d'entraÃ®nement gÃ©nÃ¨re automatiquement :

1. **Reconstructions d'images** : Comparaison originales vs reconstructions Ã  chaque epoch
2. **Visualisation du latent space** :
   - Projection PCA 2D et 3D
   - Projection t-SNE 2D et 3D
3. **GÃ©nÃ©ration conditionnelle** (CAAE et Hybrid) : GÃ©nÃ©ration de chiffres spÃ©cifiques par classe

**Exemple d'utilisation pour gÃ©nÃ©rer des images d'un chiffre spÃ©cifique :**

```python
from CAAE.models.decoder import Generator
from CAAE.utils_caae import one_hot_label_tensor
import torch

# Charger le modÃ¨le
netG = Generator().to(device)
netG.load_state_dict(torch.load('checkpoint_gen.pth'))
netG.eval()

# GÃ©nÃ©rer 16 images du chiffre 7
z = torch.randn(16, 128, device=device)
labels = torch.full((16,), 7, device=device)
class_ohe = one_hot_label_tensor(labels, 10, device)

with torch.no_grad():
    images = netG(z, class_ohe)
```

---

## 5. ğŸ“„ Document de RÃ©fÃ©rence Technique

### ğŸ‘©â€ğŸ’» PrÃ©sentation

Ce document dÃ©crit l'architecture des trois variantes d'Adversarial Autoencoders implÃ©mentÃ©es pour l'apprentissage de reprÃ©sentations latentes robustes sur MNIST.  
Il dÃ©finit les bonnes pratiques de dÃ©veloppement, l'organisation du code, la configuration des hyperparamÃ¨tres, ainsi que les rÃ¨gles Ã  respecter pour contribuer au projet.

### âœ… Objectifs

- Avoir un cadre clair et homogÃ¨ne pour le dÃ©veloppement des modÃ¨les gÃ©nÃ©ratifs
- Assurer la lisibilitÃ©, maintenabilitÃ© et reproductibilitÃ© du code
- Faciliter l'expÃ©rimentation avec diffÃ©rentes architectures
- Optimiser la collaboration et le partage des rÃ©sultats

### ğŸ§± Structure du Code

Chaque modÃ¨le (AAE, CAAE, Hybrid CAAE) suit la mÃªme organisation :

```
ModelName/
â”œâ”€â”€ config_*.py          # Tous les hyperparamÃ¨tres centralisÃ©s
â”œâ”€â”€ models/              # Architectures des rÃ©seaux
â”‚   â”œâ”€â”€ encoder.py
â”‚   â”œâ”€â”€ decoder.py
â”‚   â”œâ”€â”€ discriminator_img.py
â”‚   â””â”€â”€ discriminator_z.py
â”œâ”€â”€ losses_*.py          # DÃ©finition des fonctions de perte
â”œâ”€â”€ train_*.py           # Boucle d'entraÃ®nement principale
â””â”€â”€ utils_*.py           # Fonctions utilitaires (affichage, one-hot, init)
```

**RÃ¨gles d'organisation :**

- **SÃ©paration des prÃ©occupations** : Chaque fichier a une responsabilitÃ© unique
- **ModularitÃ©** : Les modÃ¨les peuvent Ãªtre importÃ©s et rÃ©utilisÃ©s facilement
- **Configuration centralisÃ©e** : Tous les hyperparamÃ¨tres dans `config_*.py`
- **Pas de duplication** : Le `dataset.py` commun est partagÃ© par tous les modÃ¨les

### ğŸ§‘â€ğŸ’» RÃ¨gles de DÃ©veloppement

| RÃ¨gle | Description |
|-------|-------------|
| âœï¸ **Langue** | Anglais pour le code (classes, variables, commentaires). FranÃ§ais acceptÃ© pour la documentation. |
| ğŸ§  **ClartÃ©** | Bien comprendre l'architecture avant de modifier le code. |
| ğŸ“ **Commentaires** | Commentez les parties non-Ã©videntes, surtout dans les losses et les architectures. |
| ğŸ§© **Nomination** | Respectez les conventions Python : `snake_case` pour variables/fonctions, `PascalCase` pour classes. |
| ğŸ” **ExpÃ©rimentation** | Documentez vos expÃ©rimentations (hyperparamÃ¨tres testÃ©s, rÃ©sultats) dans un fichier `experiments.md`. |
| ğŸ¤ **ReproductibilitÃ©** | Fixez les seeds alÃ©atoires (`torch.manual_seed()`) pour garantir la reproductibilitÃ©. |

### âš™ï¸ Fichiers de Configuration

#### config_*.py

Chaque modÃ¨le possÃ¨de son propre fichier de configuration. Exemple pour AAE :

```python
# config_aae.py
import torch

# Architecture
n_channel = 1        # MNIST = grayscale
n_disc = 64          # Canaux discriminateur
n_gen = 64           # Canaux gÃ©nÃ©rateur
n_encode = 64        # Canaux encodeur
n_z = 128            # Dimension latente

# Training
img_size = 32
batchSize = 64
niter = 20
lr_e = 0.0002        # Learning rate encodeur
lr_g = 0.0002        # Learning rate gÃ©nÃ©rateur
lr_d = 0.0001        # Learning rate discriminateurs

# Environnement
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
outf = './result_aae'
data_root = './data'
```

**âš ï¸ Important :**
- Ne jamais hardcoder des chemins absolus
- Utiliser `os.path.join()` pour la compatibilitÃ© multi-plateforme
- Documenter chaque hyperparamÃ¨tre

### ğŸ”¬ Fonctions de Perte

#### AAE
```python
L_total = L1(x_recon, x_original) 
          + Î»_img * BCE(D_img(x_recon), 1) 
          + Î»_z * BCE(D_z(z), 1)
```

#### CAAE
```python
L_total = MSE(x_recon, x_original)
          + Î»_adv * BCE(D_img(x_gen), 1)
          + Î»_z * BCE(D_z(z), 1)
          + CrossEntropy(y_pred, y_true)
          + Î² * KL(q(z|x) || p(z))
```

#### Hybrid CAAE
```python
L_total = L1(x_recon, x_original)
          + Î»_img * BCE(D_img(x_recon), 1)
          + Î»_z * BCE(D_z(z|y), 1)
          + CrossEntropy(y_pred, y_true)
```

### ğŸ§ª Bonnes Pratiques SupplÃ©mentaires

1. **Initialisation des poids** : Utiliser `weights_init()` avec `nn.init.normal_()` pour stabilitÃ©
2. **Label smoothing** : Utiliser 0.9 pour real, 0.1 pour fake (stabilise l'entraÃ®nement GAN)
3. **Gradient clipping** : Ã‰viter les explosions de gradients
4. **Monitoring** : Logger toutes les losses sÃ©parÃ©ment avec `tqdm`
5. **Checkpointing** : Sauvegarder les modÃ¨les rÃ©guliÃ¨rement avec `torch.save()`
6. **Visualisation** : Afficher les reconstructions Ã  chaque epoch pour dÃ©tecter les problÃ¨mes
7. **Memory management** : Appeler `torch.cuda.empty_cache()` et `gc.collect()` aprÃ¨s chaque epoch

### ğŸ“Š MÃ©triques d'Ã‰valuation

Pour chaque modÃ¨le, suivez ces mÃ©triques :

- **Loss de reconstruction** : L1 ou MSE entre images originales et reconstruites
- **Loss adversarial** : Performance des discriminateurs (D_img et D_z)
- **Accuracy de classification** (CAAE/Hybrid) : PrÃ©cision sur les labels prÃ©dits
- **KL divergence** (CAAE) : Distance entre distribution encodÃ©e et prior
- **QualitÃ© visuelle** : Inspection manuelle des reconstructions et gÃ©nÃ©rations

### ğŸ”„ Workflow de Contribution

1. **CrÃ©er une branche** pour votre expÃ©rimentation :
   ```bash
   git checkout -b experiment/nouveau-modele
   ```

2. **DÃ©velopper** en suivant la structure existante

3. **Tester** votre code avec un petit nombre d'epochs

4. **Documenter** vos rÃ©sultats dans `experiments.md`

5. **Commit** avec des messages clairs :
   ```bash
   git commit -m "feat(CAAE): Add KL annealing for better convergence"
   ```

6. **Push** et crÃ©er une Pull Request :
   ```bash
   git push origin experiment/nouveau-modele
   ```

### ğŸ“š Ressources et RÃ©fÃ©rences

**Papers fondateurs :**
- Makhzani et al. (2015) - Adversarial Autoencoders
- Kingma & Welling (2013) - Auto-Encoding Variational Bayes
- Goodfellow et al. (2014) - Generative Adversarial Networks

**Documentation PyTorch :**
- [torch.nn](https://pytorch.org/docs/stable/nn.html)
- [torch.optim](https://pytorch.org/docs/stable/optim.html)
- [torchvision.transforms](https://pytorch.org/vision/stable/transforms.html)

### ğŸ“¥ Besoin d'aide ?

Pour toute question sur :
- L'architecture des modÃ¨les
- Les hyperparamÃ¨tres optimaux
- Les problÃ¨mes d'entraÃ®nement
- L'ajout de nouvelles fonctionnalitÃ©s

Ouvrez une issue sur GitHub ou contactez les mainteneurs du projet.

---

**DÃ©veloppÃ© avec â¤ï¸ et PyTorch**